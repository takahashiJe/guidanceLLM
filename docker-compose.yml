# docker-compose.yml
# ベース定義（開発/本番共通）。起動順と依存関係、GPUの割当方針をここで統一。
services:
  # =========================
  # 依存ミドルウェア
  # =========================
  db:
    build:
      context: .
      dockerfile: Dockerfile.db
    # image: pgvector/pgvector:pg15
    container_name: ${COMPOSE_PROJECT_NAME:-guidancellm}-db-1
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-guidance}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes: [pgdata:/var/lib/postgresql/data]
    networks: [default]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 3s
      retries: 20
    profiles: ["*", "api", "worker"]

  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 10
    networks: [default]
    profiles: ["*", "api", "worker"]

  # =========================
  # GPU を使うサービス（LLMのみ）
  # =========================
  ollama:
    image: ollama/ollama:latest
    env_file: .env
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    gpus: all
    networks: [default]
    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
    profiles: ["worker"]

  # =========================
  # OSRM（日本全域データ）
  # =========================
  osrm-car:
    image: osrm/osrm-backend:latest
    ports: ["5000:5000"]
    volumes:
      - ./backend/worker/data/map/car:/data
    command: osrm-routed --algorithm mld /data/japan-latest.osrm
    networks: [default]
    profiles: ["worker"]

  osrm-foot:
    image: osrm/osrm-backend:latest
    ports: ["5001:5000"]
    volumes:
      - ./backend/worker/data/map/foot:/data
    command: osrm-routed --algorithm mld /data/japan-latest-foot.osrm
    networks: [default]
    profiles: ["worker"]

  # =========================
  # アプリ本体
  # =========================
  api_gateway:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    env_file: .env
    environment:
      - PYTHONPATH=/app/backend
      - APP_ENV=${APP_ENV}
      - LOG_LEVEL=${LOG_LEVEL}
      - CORS_ALLOW_ORIGINS=${CORS_ALLOW_ORIGINS}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      db-init:
        condition: service_completed_successfully
    networks: [default]
    profiles: ["api"]

  worker:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    env_file: .env
    environment:
      - PYTHONPATH=/app/backend
      - APP_ENV=${APP_ENV}
      - LOG_LEVEL=${LOG_LEVEL}
      - OLLAMA_HOST=${OLLAMA_HOST}
      # ※ GPU は割り当てない（NVIDIA_* も指定しない）
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
      vectorstore-init:
        condition: service_completed_successfully
      osrm-car:
        condition: service_started
      osrm-foot:
        condition: service_started
      db-init:
        condition: service_completed_successfully
    networks: [default]
    profiles: ["worker"]

  # Celery Beat（定期実行：MVリフレッシュ等）
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    env_file: .env
    environment:
      - PYTHONPATH=/app/backend
      - APP_ENV=${APP_ENV}
      - LOG_LEVEL=${LOG_LEVEL}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks: [default]
    profiles: ["worker"]

  frontend:
    build:
      context: ./frontend
    ports: ["8501:8501"]
    depends_on:
      api_gateway:
        condition: service_started
    networks: [default]
    profiles: ["dev", "api"]

  # =========================
  # 初期化ツール（起動順固定）
  # =========================
  db-init:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: ${COMPOSE_PROJECT_NAME:-guidancellm}-db-init-1
    env_file:
      - .env
    working_dir: /app
    environment:
      DATABASE_URL: "postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@db:${POSTGRES_PORT:-5432}/${POSTGRES_DB:-guidance}"
      PYTHONPATH: "/app/backend"
      ALEMBIC_SCRIPT_LOCATION: "${ALEMBIC_SCRIPT_LOCATION:-backend/shared/app/migrations}"
      ALEMBIC_DB_URL: "${DATABASE_URL}"
      INIT_RUN_ALEMBIC: "${INIT_RUN_ALEMBIC:-true}"
      INIT_LOAD_SPOTS: "${INIT_LOAD_SPOTS:-true}"
      INIT_LOAD_ACCESS_POINTS: "${INIT_LOAD_ACCESS_POINTS:-true}"
      INIT_BUILD_VECTORSTORE: "false"  # Vectorstoreは別サービスで実行
    volumes:
      - .:/app
      - ./backend:/app/backend
    depends_on:
      db:
        condition: service_healthy
    command: >
      bash -lc "
        pip install --no-cache-dir alembic &&
        python backend/worker/app/init_db_script.py
      "
    networks: [default]
    profiles: ["*", "api", "worker"]
    restart: "no"

  vectorstore-init:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    image: ${COMPOSE_PROJECT_NAME:-guidancellm}-vectorstore-init
    container_name: ${COMPOSE_PROJECT_NAME:-guidancellm}-vectorstore-init-1
    env_file: .env
    environment:
      PYTHONUNBUFFERED: "1"
      PYTHONPATH: "/app/backend"
      OLLAMA_HOST: "${OLLAMA_HOST:-http://ollama:11434}"
      EMBED_MODEL: "${EMBED_MODEL:-mxbai-embed-large}"
      KNOWLEDGE_LANGS: "${KNOWLEDGE_LANGS:-ja,en,zh}"
      KNOWLEDGE_BASE_DIR: "/app/backend/worker/app/data/knowledge"
      VECTORSTORE_BASE_DIR: "/app/backend/worker/app/data/vectorstore"
      CHROMA_TELEMETRY_DISABLED: "1"
    command: >
      bash -lc "pip install langchain-community && python /app/backend/scripts/01_build_knowledge_graph.py"
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
      db-init:
        condition: service_completed_successfully
    volumes:
      - ./backend:/app/backend
    networks: [default]
    profiles: ["*", "worker"]
    restart: "no"

volumes:
  ollama_data:
  pgdata:

networks:
  default:
    name: ${COMPOSE_PROJECT_NAME:-guidancellm}_default
